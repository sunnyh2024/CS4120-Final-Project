{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Model\n",
    "\n",
    "This is the first model we tried. It treats the playlist as a sentence, with the title going first, and generates based on that. The code is basically the code we \n",
    "used for HW3, and we chose n = 5 for our n-gram. This can be changed in the second to last code box, by changing the ngram passed into the language model on initialization\n",
    "\n",
    "To use this model, change the ngram if desired, as well as the title of the playlist to generate with in the last code box, then run all cells in the notebook. After the first run of the notebook, new playlists can be generated just by editing and running the last code box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "from spotify import SpotifyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "  \"\"\"Creates n-grams for the given token sequence.\n",
    "  Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "  Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "  \"\"\"\n",
    "  return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def read_file(path: str) -> list:\n",
    "  \"\"\"\n",
    "  Reads the contents of a file in line by line.\n",
    "  Args:\n",
    "    path (str): the location of the file to read\n",
    "\n",
    "  Returns:\n",
    "    list: list of strings, the contents of the file\n",
    "  \"\"\"\n",
    "  f = open(path)\n",
    "\n",
    "  js = json.load(f)\n",
    "  playlists = js['playlists']\n",
    "  sentences = []\n",
    "  trackID_to_name = {}\n",
    "\n",
    "  for playlist in playlists:\n",
    "    if not playlist['tracks'] or 'name' not in playlist:\n",
    "      continue\n",
    "    sentences.append(f\"{playlist['name']} {' '.join(track['track_uri'] for track in playlist['tracks'])}\")\n",
    "    for track in playlist['tracks']:\n",
    "        trackID_to_name[track['track_uri']] = track['track_name']\n",
    "  return sentences, trackID_to_name\n",
    "\n",
    "def tokenize_line(line: str, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize a single string. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    line (str): text to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - a single line tokenized\n",
    "  \"\"\"\n",
    "  inner_pieces = None\n",
    "  if by_char:\n",
    "    inner_pieces = list(line)\n",
    "  else:\n",
    "    # otherwise split on white space\n",
    "    inner_pieces = line.split()\n",
    "\n",
    "  if ngram == 1:\n",
    "    tokens = [sentence_begin] + inner_pieces + [sentence_end]\n",
    "  else:\n",
    "    tokens = ([sentence_begin] * (ngram - 1)) + inner_pieces + ([sentence_end] * (ngram - 1))\n",
    "  # always count the unigrams\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def tokenize(data: list, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize each line in a list of strings. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    data (list): list of strings to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - all lines tokenized as one large list\n",
    "  \"\"\"\n",
    "  total = []\n",
    "  # also glue on sentence begin and end items\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    # skip empty lines\n",
    "    if len(line) == 0:\n",
    "      continue\n",
    "    tokens = tokenize_line(line, ngram, by_char, sentence_begin, sentence_end)\n",
    "    total += tokens\n",
    "  return total\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "\n",
    "  def __init__(self, n_gram):\n",
    "    \"\"\"Initializes an untrained LanguageModel\n",
    "    Args:\n",
    "      n_gram (int): the n-gram order of the language model to create\n",
    "    \"\"\"\n",
    "    self.n = n_gram\n",
    "    self.prob_matrix = {}\n",
    "    self.generate_matrix = {}\n",
    "\n",
    "  \n",
    "  def train(self, tokens: list, verbose: bool = False) -> None:\n",
    "    \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "    has tokens that are white-space separated, has one sentence per line, and\n",
    "    that the sentences begin with <s> and end with </s>\n",
    "    Args:\n",
    "      tokens (list): tokenized data to be trained on as a single list\n",
    "      verbose (bool): default value False, to be used to turn on/off debugging prints\n",
    "    \"\"\"\n",
    "    # initialize counter and remove the unknowns\n",
    "    self.counts = Counter(tokens)\n",
    "    # replace unknown tokens with UNKS\n",
    "    tokens = [token if self.counts[token] > 1 else UNK for token in tokens]\n",
    "    unks = []\n",
    "    for tok in self.counts:\n",
    "      if self.counts[tok] == 1:\n",
    "        unks.append(tok)\n",
    "    if unks:\n",
    "      for unk in unks:\n",
    "        del self.counts[unk]\n",
    "      self.counts[UNK] = len(unks)\n",
    "\n",
    "    self.vocab_len = len(self.counts)\n",
    "    n_grams = create_ngrams(tokens, self.n)\n",
    "    self.n_gram_counts = Counter(n_grams)\n",
    "\n",
    "    # if n is 1, then just use the counts created above\n",
    "    if self.n == 1:\n",
    "      for tok in n_grams:\n",
    "        self.prob_matrix[tok[0]] = (self.counts[tok[0]] + 1) / (len(tokens) + self.vocab_len) # laplace smoothing\n",
    "    # if n > 1, calculate and save the probability of ngram | (n-1)gram\n",
    "    else:\n",
    "      self.n_minus_1_counts = Counter(create_ngrams(tokens, self.n - 1))\n",
    "      for ngram in self.n_gram_counts:\n",
    "        # laplace here too\n",
    "        self.prob_matrix[ngram] = (self.n_gram_counts[ngram] + 1) / (self.n_minus_1_counts[ngram[:-1]] + self.vocab_len)\n",
    "\n",
    "\n",
    "  def score(self, sentence_tokens: list) -> float:\n",
    "    \"\"\"Calculates the probability score for a given string representing a single sequence of tokens.\n",
    "    Args:\n",
    "      sentence_tokens (list): a tokenized sequence to be scored by this model\n",
    "      \n",
    "    Returns:\n",
    "      float: the probability value of the given tokens for this model\n",
    "    \"\"\"\n",
    "    # replace unknowns in the sentence with UNKS\n",
    "    sentence_tokens = [tok if tok in self.counts and self.counts[tok] > 1 else UNK for tok in sentence_tokens]\n",
    "    probability = 1\n",
    "    if self.n == 1:\n",
    "      for tok in sentence_tokens:\n",
    "        # get probability and multiply\n",
    "        probability *= self.prob_matrix[tok]\n",
    "    else:\n",
    "      sentence_ngrams = create_ngrams(sentence_tokens, self.n)\n",
    "      for ngram in sentence_ngrams:\n",
    "        # handling cases where the ngram does not appear in the training data\n",
    "        if ngram not in self.n_gram_counts:\n",
    "          denom = self.vocab_len\n",
    "          if ngram[:-1] in self.n_minus_1_counts:\n",
    "            denom += self.n_minus_1_counts[ngram[:-1]]\n",
    "          cur_prob = 1 / denom\n",
    "        else:\n",
    "          # already applied laplace in train so no need here\n",
    "          cur_prob = self.prob_matrix[ngram]\n",
    "        probability *= cur_prob\n",
    "    return probability\n",
    "  \n",
    "  \n",
    "  def generate_sentence(self, starter_input) -> list:\n",
    "    \"\"\"Generates a single sentence from a trained language model using the Shannon technique.\n",
    "      \n",
    "    Returns:\n",
    "      list: the generated sentence as a list of tokens\n",
    "    \"\"\"\n",
    "    # for models where n > 2, create a <s> padding so we start with a valid (n-1)-gram\n",
    "    # sentence = [SENTENCE_BEGIN] + [SENTENCE_BEGIN for _ in range(self.n - 2)]\n",
    "    sentence = [SENTENCE_BEGIN for _ in range(self.n - 2)] + starter_input\n",
    "    \n",
    "    def generate_next_word(sentence):\n",
    "      if sentence[-1] == SENTENCE_END:\n",
    "        return sentence\n",
    "      \n",
    "      # get the last (n-1)-gram in the sentence\n",
    "      cur_n_minus_1_gram = tuple(sentence[-self.n + 1:])\n",
    "\n",
    "      next_word = SENTENCE_BEGIN\n",
    "      while next_word == SENTENCE_BEGIN: # to make sure we don't select <s>\n",
    "        if self.n == 1:\n",
    "          next_word = np.random.choice(list(self.prob_matrix.keys()), p=list(self.prob_matrix.values()))\n",
    "        else:\n",
    "          # find the different options for next word\n",
    "          options = []\n",
    "          cur_probs = []\n",
    "          for ngram in self.n_gram_counts:\n",
    "            # we found a matching (n-1)-gram, add the last word as an option\n",
    "            if ngram[:-1] == cur_n_minus_1_gram:\n",
    "              options.append(ngram[-1])\n",
    "              cur_probs.append(self.prob_matrix[ngram])\n",
    "          cur_probs = [p / sum(cur_probs) for p in cur_probs] # normalizing probabilities\n",
    "          next_word = np.random.choice(options, p=cur_probs)\n",
    "\n",
    "      sentence.append(next_word)\n",
    "      return generate_next_word(sentence)\n",
    "    \n",
    "    return generate_next_word(sentence)\n",
    "  \n",
    "\n",
    "  def generate(self, n: int) -> list:\n",
    "    \"\"\"Generates n sentences from a trained language model using the Shannon technique.\n",
    "    Args:\n",
    "      n (int): the number of sentences to generate\n",
    "      \n",
    "    Returns:\n",
    "      list: a list containing lists of strings, one per generated sentence\n",
    "    \"\"\"\n",
    "    # PROVIDED\n",
    "    return [self.generate_sentence() for i in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data and tokenize the sentences\n",
    "data, trackID_to_name = read_file('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "tokens = tokenize(data, 5, False)\n",
    "\n",
    "# initiate a language model and train on the generated tokens\n",
    "model = LanguageModel(5)\n",
    "model.train(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = read_file('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "tokens = tokenize(data, 5, by_char=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spotify:track:74BIr8H03lr5L6LZ1friIa', 'spotify:track:1OjiYBvBYSKwLQP2bryPYw', 'spotify:track:5QfQKhvdcxPgp68O0S39FV', 'spotify:track:4qYLPta5HZ36idWiXtqh7B', 'spotify:track:6g0TAJ1grweRwBMLQyiZFb', 'spotify:track:0ORPtlRSmDasaKqmgnuMbC', 'spotify:track:3DZQ6mzUkAdHqZWzqxBKIK', 'spotify:track:05nbZ1xxVNwUTcGwLbp7CN', 'spotify:track:0Dc7J9VPV4eOInoxUiZrsL', 'spotify:track:6yHkPtl6UQ7RjtJLBPzbJw', 'spotify:track:55GLc4nywcX4aIlOcx1u06', 'spotify:track:4AFgK4wP1iD5i8BYaLr9Vf', 'spotify:track:3OGuiEDR8XXjaWqbOaVqUA', 'spotify:track:67W5fd1ld7dqHNCfaQu52I', 'spotify:track:2eAZfqOm4EnOF9VvN50Tyc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['spotify:track:74BIr8H03lr5L6LZ1friIa',\n",
       "  'spotify:track:1OjiYBvBYSKwLQP2bryPYw',\n",
       "  'spotify:track:5QfQKhvdcxPgp68O0S39FV',\n",
       "  'spotify:track:4qYLPta5HZ36idWiXtqh7B',\n",
       "  'spotify:track:6g0TAJ1grweRwBMLQyiZFb',\n",
       "  'spotify:track:0ORPtlRSmDasaKqmgnuMbC',\n",
       "  'spotify:track:3DZQ6mzUkAdHqZWzqxBKIK',\n",
       "  'spotify:track:05nbZ1xxVNwUTcGwLbp7CN',\n",
       "  'spotify:track:0Dc7J9VPV4eOInoxUiZrsL',\n",
       "  'spotify:track:6yHkPtl6UQ7RjtJLBPzbJw',\n",
       "  'spotify:track:55GLc4nywcX4aIlOcx1u06',\n",
       "  'spotify:track:4AFgK4wP1iD5i8BYaLr9Vf',\n",
       "  'spotify:track:3OGuiEDR8XXjaWqbOaVqUA',\n",
       "  'spotify:track:67W5fd1ld7dqHNCfaQu52I',\n",
       "  'spotify:track:2eAZfqOm4EnOF9VvN50Tyc'],\n",
       " ['Dead People',\n",
       "  'Time',\n",
       "  'Dan Bilzerian',\n",
       "  'Highlights',\n",
       "  'Famous',\n",
       "  'There She Go',\n",
       "  'Loveeeeeee Song',\n",
       "  'Myself',\n",
       "  \"Don't Tell 'Em\",\n",
       "  'Selfish',\n",
       "  'FaceTime',\n",
       "  'You Da Baddest',\n",
       "  'Once Upon a Time',\n",
       "  'Dance (A$$)',\n",
       "  'The Way Life Goes (feat. Oh Wonder)'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates a playlist with the given title, capped at max_length\n",
    "def generate_playlist(playlist_title: str, max_length: int = 50):\n",
    "    playlist = model.generate_sentence([SENTENCE_BEGIN, SENTENCE_BEGIN] + playlist_title.split())\n",
    "    playlist = [track for track in set(playlist) if track.startswith('spotify:track:')][:max_length]\n",
    "    print(playlist)\n",
    "\n",
    "    track_names = []\n",
    "    for track_id in playlist:\n",
    "        track_names.append(trackID_to_name.get(track_id, \"Unknown Track Name\"))\n",
    "    return playlist, track_names\n",
    "    \n",
    "# change the params here to generate for other playlist titles/lengths\n",
    "generate_playlist(\"chill vibes\", 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
