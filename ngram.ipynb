{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "from spotify import SpotifyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "  \"\"\"Creates n-grams for the given token sequence.\n",
    "  Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "  Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "  \"\"\"\n",
    "  return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def read_file(path: str) -> list:\n",
    "  \"\"\"\n",
    "  Reads the contents of a file in line by line.\n",
    "  Args:\n",
    "    path (str): the location of the file to read\n",
    "\n",
    "  Returns:\n",
    "    list: list of strings, the contents of the file\n",
    "  \"\"\"\n",
    "  f = open(path)\n",
    "\n",
    "  js = json.load(f)\n",
    "  playlists = js['playlists']\n",
    "  sentences = []\n",
    "\n",
    "  for playlist in playlists:\n",
    "    if not playlist['tracks'] or 'name' not in playlist:\n",
    "      continue\n",
    "    sentences.append(f\"{playlist['name']} {' '.join(track['track_uri'] for track in playlist['tracks'])}\")\n",
    "  return sentences\n",
    "\n",
    "def tokenize_line(line: str, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize a single string. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    line (str): text to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - a single line tokenized\n",
    "  \"\"\"\n",
    "  inner_pieces = None\n",
    "  if by_char:\n",
    "    inner_pieces = list(line)\n",
    "  else:\n",
    "    # otherwise split on white space\n",
    "    inner_pieces = line.split()\n",
    "\n",
    "  if ngram == 1:\n",
    "    tokens = [sentence_begin] + inner_pieces + [sentence_end]\n",
    "  else:\n",
    "    tokens = ([sentence_begin] * (ngram - 1)) + inner_pieces + ([sentence_end] * (ngram - 1))\n",
    "  # always count the unigrams\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def tokenize(data: list, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize each line in a list of strings. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    data (list): list of strings to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - all lines tokenized as one large list\n",
    "  \"\"\"\n",
    "  total = []\n",
    "  # also glue on sentence begin and end items\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    # skip empty lines\n",
    "    if len(line) == 0:\n",
    "      continue\n",
    "    tokens = tokenize_line(line, ngram, by_char, sentence_begin, sentence_end)\n",
    "    total += tokens\n",
    "  return total\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "\n",
    "  def __init__(self, n_gram):\n",
    "    \"\"\"Initializes an untrained LanguageModel\n",
    "    Args:\n",
    "      n_gram (int): the n-gram order of the language model to create\n",
    "    \"\"\"\n",
    "    self.n = n_gram\n",
    "    self.prob_matrix = {}\n",
    "    self.generate_matrix = {}\n",
    "\n",
    "  \n",
    "  def train(self, tokens: list, verbose: bool = False) -> None:\n",
    "    \"\"\"Trains the language model on the given data. Assumes that the given data\n",
    "    has tokens that are white-space separated, has one sentence per line, and\n",
    "    that the sentences begin with <s> and end with </s>\n",
    "    Args:\n",
    "      tokens (list): tokenized data to be trained on as a single list\n",
    "      verbose (bool): default value False, to be used to turn on/off debugging prints\n",
    "    \"\"\"\n",
    "    # initialize counter and remove the unknowns\n",
    "    self.counts = Counter(tokens)\n",
    "    # replace unknown tokens with UNKS\n",
    "    tokens = [token if self.counts[token] > 1 else UNK for token in tokens]\n",
    "    unks = []\n",
    "    for tok in self.counts:\n",
    "      if self.counts[tok] == 1:\n",
    "        unks.append(tok)\n",
    "    if unks:\n",
    "      for unk in unks:\n",
    "        del self.counts[unk]\n",
    "      self.counts[UNK] = len(unks)\n",
    "\n",
    "    self.vocab_len = len(self.counts)\n",
    "    n_grams = create_ngrams(tokens, self.n)\n",
    "    self.n_gram_counts = Counter(n_grams)\n",
    "\n",
    "    # if n is 1, then just use the counts created above\n",
    "    if self.n == 1:\n",
    "      for tok in n_grams:\n",
    "        self.prob_matrix[tok[0]] = (self.counts[tok[0]] + 1) / (len(tokens) + self.vocab_len) # laplace smoothing\n",
    "    # if n > 1, calculate and save the probability of ngram | (n-1)gram\n",
    "    else:\n",
    "      self.n_minus_1_counts = Counter(create_ngrams(tokens, self.n - 1))\n",
    "      for ngram in self.n_gram_counts:\n",
    "        # laplace here too\n",
    "        self.prob_matrix[ngram] = (self.n_gram_counts[ngram] + 1) / (self.n_minus_1_counts[ngram[:-1]] + self.vocab_len)\n",
    "\n",
    "\n",
    "  def score(self, sentence_tokens: list) -> float:\n",
    "    \"\"\"Calculates the probability score for a given string representing a single sequence of tokens.\n",
    "    Args:\n",
    "      sentence_tokens (list): a tokenized sequence to be scored by this model\n",
    "      \n",
    "    Returns:\n",
    "      float: the probability value of the given tokens for this model\n",
    "    \"\"\"\n",
    "    # replace unknowns in the sentence with UNKS\n",
    "    sentence_tokens = [tok if tok in self.counts and self.counts[tok] > 1 else UNK for tok in sentence_tokens]\n",
    "    probability = 1\n",
    "    if self.n == 1:\n",
    "      for tok in sentence_tokens:\n",
    "        # get probability and multiply\n",
    "        probability *= self.prob_matrix[tok]\n",
    "    else:\n",
    "      sentence_ngrams = create_ngrams(sentence_tokens, self.n)\n",
    "      for ngram in sentence_ngrams:\n",
    "        # handling cases where the ngram does not appear in the training data\n",
    "        if ngram not in self.n_gram_counts:\n",
    "          denom = self.vocab_len\n",
    "          if ngram[:-1] in self.n_minus_1_counts:\n",
    "            denom += self.n_minus_1_counts[ngram[:-1]]\n",
    "          cur_prob = 1 / denom\n",
    "        else:\n",
    "          # already applied laplace in train so no need here\n",
    "          cur_prob = self.prob_matrix[ngram]\n",
    "        probability *= cur_prob\n",
    "    return probability\n",
    "  \n",
    "  \n",
    "  def generate_sentence(self, starter_input) -> list:\n",
    "    \"\"\"Generates a single sentence from a trained language model using the Shannon technique.\n",
    "      \n",
    "    Returns:\n",
    "      list: the generated sentence as a list of tokens\n",
    "    \"\"\"\n",
    "    # for models where n > 2, create a <s> padding so we start with a valid (n-1)-gram\n",
    "    # sentence = [SENTENCE_BEGIN] + [SENTENCE_BEGIN for _ in range(self.n - 2)]\n",
    "    sentence = [SENTENCE_BEGIN for _ in range(self.n - 2)] + starter_input\n",
    "    \n",
    "    def generate_next_word(sentence):\n",
    "      if sentence[-1] == SENTENCE_END:\n",
    "        return sentence\n",
    "      \n",
    "      # get the last (n-1)-gram in the sentence\n",
    "      cur_n_minus_1_gram = tuple(sentence[-self.n + 1:])\n",
    "\n",
    "      next_word = SENTENCE_BEGIN\n",
    "      while next_word == SENTENCE_BEGIN: # to make sure we don't select <s>\n",
    "        if self.n == 1:\n",
    "          next_word = np.random.choice(list(self.prob_matrix.keys()), p=list(self.prob_matrix.values()))\n",
    "        else:\n",
    "          # find the different options for next word\n",
    "          options = []\n",
    "          cur_probs = []\n",
    "          for ngram in self.n_gram_counts:\n",
    "            # we found a matching (n-1)-gram, add the last word as an option\n",
    "            if ngram[:-1] == cur_n_minus_1_gram:\n",
    "              options.append(ngram[-1])\n",
    "              cur_probs.append(self.prob_matrix[ngram])\n",
    "          cur_probs = [p / sum(cur_probs) for p in cur_probs] # normalizing probabilities\n",
    "          next_word = np.random.choice(options, p=cur_probs)\n",
    "\n",
    "      sentence.append(next_word)\n",
    "      return generate_next_word(sentence)\n",
    "    \n",
    "    return generate_next_word(sentence)\n",
    "  \n",
    "\n",
    "  def generate(self, n: int) -> list:\n",
    "    \"\"\"Generates n sentences from a trained language model using the Shannon technique.\n",
    "    Args:\n",
    "      n (int): the number of sentences to generate\n",
    "      \n",
    "    Returns:\n",
    "      list: a list containing lists of strings, one per generated sentence\n",
    "    \"\"\"\n",
    "    # PROVIDED\n",
    "    return [self.generate_sentence() for i in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<s>', '<s>', '<s>', 'pool', 'party', 'spotify:track:4WjH9Bzt3kx7z8kl0awxh4', 'spotify:track:3fDrZa4ksxA5lgi0utGu6k', 'spotify:track:0uVyZywUNOp6S0dU5r8TS8', 'spotify:track:494OU6M7NOf4ICYb4zWCf5', 'spotify:track:66hayvUbTotekKU3H4ta1f', 'spotify:track:4WjH9Bzt3kx7z8kl0awxh4', 'spotify:track:3NBDgwEAGMj0aKRsU8zoO9', 'spotify:track:6OKaib6HaebKVCL5VOoIGR', 'spotify:track:5aJhuze13LOoLthmiG8YIA', '<UNK>', 'spotify:track:7tZZgrwL3rAnrNDHLOp4QA', 'spotify:track:0wsXdby1T3PWLauIkGUZzg', 'spotify:track:3q4zNC35eIv15Zt4cEDicP', 'spotify:track:6O6M7pJLABmfBRoGZMu76Y', 'spotify:track:17Fd6Yb7mSbinKG8LoWfFl', 'spotify:track:4PLOGIceFMY5RHNLjbNaMs', 'spotify:track:0qt5f5EL92o8Snzopsv0en', 'spotify:track:6nM9Sr6MPcfpqwQTD7PZGi', 'spotify:track:6JV2JOEocMgcZxYSZelKcc', 'spotify:track:2S5LNtRVRPbXk01yRQ14sZ', 'spotify:track:0cOBMETjhxublnnwhbnzJO', 'spotify:track:7py16W5fWYLFFS6BElKAjn', 'spotify:track:3bvjsHQd792zIO4SKFxXKs', 'spotify:track:1rIKgCH4H52lrvDcz50hS8', 'spotify:track:1louJpMmzEicAn7lzDalPW', '</s>']\n"
     ]
    }
   ],
   "source": [
    "data = read_file('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "tokens = tokenize(data, 5, False)\n",
    "\n",
    "model = LanguageModel(5)\n",
    "model.train(tokens)\n",
    "playlist = model.generate_sentence([SENTENCE_BEGIN, SENTENCE_BEGIN, \"pool\", \"party\"])\n",
    "\n",
    "print(playlist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = read_file('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "tokens = tokenize(data, 5, by_char=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tracks': [{'album': {'album_type': 'single',\n",
       "    'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/5UVzX8pQe6bb5ueNdfViih'},\n",
       "      'href': 'https://api.spotify.com/v1/artists/5UVzX8pQe6bb5ueNdfViih',\n",
       "      'id': '5UVzX8pQe6bb5ueNdfViih',\n",
       "      'name': 'Chris Malinchak',\n",
       "      'type': 'artist',\n",
       "      'uri': 'spotify:artist:5UVzX8pQe6bb5ueNdfViih'}],\n",
       "    'available_markets': ['CA', 'US'],\n",
       "    'external_urls': {'spotify': 'https://open.spotify.com/album/5z6aqbVipcLm2KtFLYMhfj'},\n",
       "    'href': 'https://api.spotify.com/v1/albums/5z6aqbVipcLm2KtFLYMhfj',\n",
       "    'id': '5z6aqbVipcLm2KtFLYMhfj',\n",
       "    'images': [{'height': 640,\n",
       "      'url': 'https://i.scdn.co/image/ab67616d0000b273c9c9c8f342801ab3e62d0e43',\n",
       "      'width': 640},\n",
       "     {'height': 300,\n",
       "      'url': 'https://i.scdn.co/image/ab67616d00001e02c9c9c8f342801ab3e62d0e43',\n",
       "      'width': 300},\n",
       "     {'height': 64,\n",
       "      'url': 'https://i.scdn.co/image/ab67616d00004851c9c9c8f342801ab3e62d0e43',\n",
       "      'width': 64}],\n",
       "    'name': 'So Good to Me (Radio Edit)',\n",
       "    'release_date': '2013-11-19',\n",
       "    'release_date_precision': 'day',\n",
       "    'total_tracks': 1,\n",
       "    'type': 'album',\n",
       "    'uri': 'spotify:album:5z6aqbVipcLm2KtFLYMhfj'},\n",
       "   'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/5UVzX8pQe6bb5ueNdfViih'},\n",
       "     'href': 'https://api.spotify.com/v1/artists/5UVzX8pQe6bb5ueNdfViih',\n",
       "     'id': '5UVzX8pQe6bb5ueNdfViih',\n",
       "     'name': 'Chris Malinchak',\n",
       "     'type': 'artist',\n",
       "     'uri': 'spotify:artist:5UVzX8pQe6bb5ueNdfViih'}],\n",
       "   'available_markets': ['CA', 'US'],\n",
       "   'disc_number': 1,\n",
       "   'duration_ms': 226120,\n",
       "   'explicit': False,\n",
       "   'external_ids': {'isrc': 'GBARL1301455'},\n",
       "   'external_urls': {'spotify': 'https://open.spotify.com/track/3fDrZa4ksxA5lgi0utGu6k'},\n",
       "   'href': 'https://api.spotify.com/v1/tracks/3fDrZa4ksxA5lgi0utGu6k',\n",
       "   'id': '3fDrZa4ksxA5lgi0utGu6k',\n",
       "   'is_local': False,\n",
       "   'name': 'So Good to Me - Radio Edit',\n",
       "   'popularity': 46,\n",
       "   'preview_url': 'https://p.scdn.co/mp3-preview/fffcc24a7dacf7f25b4b191e651a303bcb8ba3b1?cid=744e625455ed47afa39d44c224c3eb34',\n",
       "   'track_number': 1,\n",
       "   'type': 'track',\n",
       "   'uri': 'spotify:track:3fDrZa4ksxA5lgi0utGu6k'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = SpotifyClient()\n",
    "client.get_song_title(playlist[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
