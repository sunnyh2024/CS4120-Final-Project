{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "\n",
    "js = json.load(f)\n",
    "playlists = js['playlists']\n",
    "titles = []\n",
    "tracks = []\n",
    "\n",
    "for playlist in playlists:\n",
    "    if not playlist['tracks'] or 'name' not in playlist:\n",
    "        continue\n",
    "    titles.append(playlist['name'].lower())\n",
    "    tracks.append(' '.join(track['track_uri'] for track in playlist['tracks']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tokens, title_tokenizer = tokenize(titles)\n",
    "tracks_tokens, track_tokenizer = tokenize(tracks)\n",
    "\n",
    "title_vocab = len(title_tokenizer.word_index) + 1\n",
    "track_vocab = len(track_tokenizer.word_index) + 1\n",
    "\n",
    "max_title_length = int(len(max(titles_tokens, key=len)))\n",
    "max_track_length = int(len(max(tracks_tokens, key=len)))\n",
    "\n",
    "pad_titles = pad_sequences(titles_tokens, max_title_length, padding = \"post\")\n",
    "pad_tracks = pad_sequences(tracks_tokens, max_track_length, padding = \"post\")\n",
    "\n",
    "pad_titles = pad_titles.reshape(*pad_titles.shape, 1)\n",
    "pad_tracks = pad_tracks.reshape(*pad_tracks.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_title_length,))\n",
    "embedding = Embedding(input_dim=title_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_track_length)(encoder)\n",
    "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n",
    "logits = TimeDistributed(Dense(track_vocab))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 9, 128)            285440    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 300, 64)           0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 300, 64)           33024     \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 300, 63999)        4159935   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300, 63999)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4527807 (17.27 MB)\n",
      "Trainable params: 4527807 (17.27 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 451s 2s/step - loss: 4.2757 - accuracy: 0.6200\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "<empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n"
     ]
    }
   ],
   "source": [
    "model_results = enc_dec_model.fit(pad_titles, pad_tracks, batch_size=30, epochs=1)\n",
    "\n",
    "def logits_to_sentence(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<empty>' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print(logits_to_sentence(enc_dec_model.predict(pad_titles[20:21])[0], track_tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
