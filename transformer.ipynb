{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranformer model with PyTorch\n",
    "\n",
    "This notebook creates and trains a tranformer with encoder-decoder architecture using PyTorch\n",
    "\n",
    "To use this model, change the seed number of the ..., then run all cells in the notebook. After the first run of the notebook and the model is trained, new playlists can be generated just by editing and running the last code box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from spotify import SpotifyClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating the model__\n",
    "\n",
    "Creating the layers for model. The multi-head attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # get different heads, and combine to output\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Encoder Layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decoder Layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, title_mask, track_mask):\n",
    "        attn_output = self.self_attn(x, x, x, track_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, title_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, title_vocab_size, track_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(title_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(track_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, track_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        title_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        track_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        track_mask = track_mask & nopeak_mask\n",
    "        return title_mask, track_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        title_mask, track_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        enc_output = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, title_mask)\n",
    "\n",
    "        dec_output = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, title_mask, track_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading in the data from the challenge set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch size for our experiments\n",
    "BATCH_SIZE = 35\n",
    "# define a percentage of the data to use for training\n",
    "SPLIT_PC = .90\n",
    "\n",
    "# open the file and convert it to json\n",
    "f = open('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "js = json.load(f)\n",
    "playlists = js['playlists']\n",
    "\n",
    "titles = []\n",
    "tracks = []\n",
    "\n",
    "# process and add the playlist names and tracks to lists\n",
    "for playlist in playlists:\n",
    "    if not playlist['tracks'] or 'name' not in playlist:\n",
    "        continue\n",
    "    titles.append(playlist['name'].lower()) \n",
    "    tracks.append(' '.join(track['track_uri'] for track in playlist['tracks']))\n",
    "\n",
    "END = int(len(titles)*SPLIT_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tokenizer for separating tracks (just separates by spaces)\n",
    "# we tried using tokenizers from libraries, but they split up the track URIs, which we didn't want\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "\n",
    "        # Add the padding token\n",
    "        self.dictionary['<UNK>'] = 0\n",
    "        self.reverse_dictionary[0] = '<UNK>'\n",
    "        self.cur_token = 1\n",
    "\n",
    "    # adds tokens from a list of sentences to the tokenizer\n",
    "    def fit_on_texts(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self.tokenize(sentence)\n",
    "\n",
    "    # add tokens from a sentence to the tokenizer, but does not tokenize the sentence\n",
    "    def tokenize(self, text: str):\n",
    "        for token in text.split():\n",
    "            if token not in self.dictionary:\n",
    "                self.dictionary[token] = self.cur_token\n",
    "                self.reverse_dictionary[self.cur_token] = token\n",
    "                self.cur_token += 1\n",
    "\n",
    "    # returns the token associated with the given character/word, or None if the character does not exist\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary.get(character, None)\n",
    "\n",
    "    # returns the character/word associated with the given token, or None if the token does not exist\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary.get(token, None)\n",
    "    \n",
    "    # tokenizes a list of sentences, returning the tokens\n",
    "    def texts_to_sequences(self, sentences):\n",
    "        sequences = []\n",
    "        for sentence in sentences:\n",
    "            cur_sentence = []\n",
    "            for token in sentence.split():\n",
    "                if token not in self.dictionary:\n",
    "                    cur_sentence.append(self.dictionary['<UNK>'])\n",
    "                else:\n",
    "                    cur_sentence.append(self.dictionary[token])\n",
    "            sequences.append(cur_sentence)\n",
    "        return sequences\n",
    "    \n",
    "    # returns the characters/words from a sequence of tokens\n",
    "    def sequence_to_texts(self, sequence: list):\n",
    "        return [self.token_to_character(token) for token in sequence]\n",
    "\n",
    "    # returns the size of the tokenizer vocab\n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize and pad titles and tracks__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes the given sentences and returns a list of lists of the tokens\n",
    "def tokenize(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer.texts_to_sequences(sentences), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3, 4], [5, 6], [7], [8, 9], [10], [11], [12, 13], [14]]\n",
      "Vocabularies: 2487, 63998\n",
      "Max lengths: 9, 100\n",
      "Example paddings: [[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 2  0  0  0  0  0  0  0  0]\n",
      " [ 3  4  0  0  0  0  0  0  0]\n",
      " [ 5  6  0  0  0  0  0  0  0]\n",
      " [ 7  0  0  0  0  0  0  0  0]\n",
      " [ 8  9  0  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0]\n",
      " [12 13  0  0  0  0  0  0  0]\n",
      " [14  0  0  0  0  0  0  0  0]]\n",
      "Tensor shapes: torch.Size([7000, 9]), torch.Size([7000, 100])\n"
     ]
    }
   ],
   "source": [
    "# tokenize titles and tracks, and also get the tokenizers\n",
    "titles_tokens, title_tokenizer = tokenize(titles)\n",
    "tracks_tokens, track_tokenizer = tokenize(tracks)\n",
    "print(titles_tokens[:10])\n",
    "\n",
    "title_vocab = title_tokenizer.size() + 1\n",
    "track_vocab = track_tokenizer.size() + 1\n",
    "print(f'Vocabularies: {title_vocab}, {track_vocab}')\n",
    "\n",
    "max_title_length = int(len(max(titles_tokens, key=len)))\n",
    "max_track_length = int(len(max(tracks_tokens, key=len)))\n",
    "print(f'Max lengths: {max_title_length}, {max_track_length}')\n",
    "\n",
    "pad_titles = pad_sequences(titles_tokens, max_title_length, padding = \"post\")\n",
    "pad_tracks = pad_sequences(tracks_tokens, max_track_length, padding = \"post\")\n",
    "print(f'Example paddings: {pad_titles[:10]}')\n",
    "\n",
    "title_tensor = torch.LongTensor(pad_titles)\n",
    "tracks_tensor = torch.LongTensor(pad_tracks)\n",
    "print(f'Tensor shapes: {title_tensor.shape}, {tracks_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a data generator for training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator for title and track tensors\n",
    "def data_generator(titles: torch.LongTensor, tracks: torch.LongTensor, batch_size: int = BATCH_SIZE):\n",
    "    i = 0\n",
    "    while True:\n",
    "        # return the current batch\n",
    "        yield titles[i: i + batch_size], tracks[i: i + batch_size]\n",
    "        \n",
    "        i += batch_size\n",
    "        # if we reach the end of the tensor, start from the beginning again\n",
    "        if i >= len(titles):\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(2487, 512)\n",
       "  (decoder_embedding): Embedding(63998, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=63998, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# initialize a Transformer model (from above) using the declared params\n",
    "transformer = Transformer(title_vocab, track_vocab, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__\n",
    "\n",
    "This took about 30 sec for 10 epochs with batch size of 35 on a M2 Macbook Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 11.189005851745605\n",
      "Epoch: 2, Loss: 11.276976585388184\n",
      "Epoch: 3, Loss: 11.185345649719238\n",
      "Epoch: 4, Loss: 11.214712142944336\n",
      "Epoch: 5, Loss: 11.159171104431152\n",
      "Epoch: 6, Loss: 11.037936210632324\n",
      "Epoch: 7, Loss: 11.10622787475586\n",
      "Epoch: 8, Loss: 11.050960540771484\n",
      "Epoch: 9, Loss: 11.041683197021484\n",
      "Epoch: 10, Loss: 10.912346839904785\n",
      "Epoch: 11, Loss: 11.091769218444824\n",
      "Epoch: 12, Loss: 11.01407527923584\n",
      "Epoch: 13, Loss: 11.002462387084961\n",
      "Epoch: 14, Loss: 10.9982328414917\n",
      "Epoch: 15, Loss: 10.865768432617188\n",
      "Epoch: 16, Loss: 10.556499481201172\n",
      "Epoch: 17, Loss: 10.712580680847168\n",
      "Epoch: 18, Loss: 10.907352447509766\n",
      "Epoch: 19, Loss: 10.831652641296387\n",
      "Epoch: 20, Loss: 10.94340705871582\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# set the model to train mode and create generator for training\n",
    "transformer.train()\n",
    "generator = data_generator(title_tensor, tracks_tensor)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    titles, tracks = next(generator)\n",
    "    output = transformer(titles, tracks)\n",
    "    loss = criterion(output.contiguous().view(-1, track_vocab), tracks.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluating and generating predictions__\n",
    "\n",
    "Currently, the model is only returning the same song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating playlist: wedding playlist\n",
      "['One Dance by Drake, Wizkid, Kyla']\n"
     ]
    }
   ],
   "source": [
    "# Change the model to eval mode\n",
    "transformer.eval()\n",
    "\n",
    "def generate_playlist(seed: int):\n",
    "    print('Generating playlist:', ' '.join([token for token in title_tokenizer.sequence_to_texts(title_tensor[seed].tolist()) if token != \"<UNK>\"]))\n",
    "    output = transformer(title_tensor[seed: seed + 1], tracks_tensor[seed: seed + 1])\n",
    "\n",
    "    # Get the track with the highest probability, then convert the tokens back to URIs\n",
    "    output = output.view(-1, track_vocab).argmax(1)\n",
    "    output = track_tokenizer.sequence_to_texts(output.numpy())\n",
    "\n",
    "    output = list(set(output))\n",
    "\n",
    "    # use Spotify API to get the song names\n",
    "    client = SpotifyClient()\n",
    "    playlist = client.get_song_titles(output)\n",
    "    print([f\"{track['name']} by {', '.join([artist['name'] for artist in track['artists']])}\" for track in playlist['tracks']])\n",
    "\n",
    "# change to generate for a different seed\n",
    "generate_playlist(151)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
