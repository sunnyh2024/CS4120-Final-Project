{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shash\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('spotify_million_playlist_dataset_challenge/challenge_set.json')\n",
    "\n",
    "js = json.load(f)\n",
    "playlists = js['playlists']\n",
    "titles = []\n",
    "tracks = []\n",
    "trackID_to_name = {}\n",
    "\n",
    "for playlist in playlists:\n",
    "    if not playlist['tracks'] or 'name' not in playlist:\n",
    "        continue\n",
    "    titles.append(playlist['name'].lower())\n",
    "    tracks.append(' '.join(track['track_uri'].split(\":\")[2] for track in playlist['tracks']))\n",
    "    for track in playlist['tracks']:\n",
    "        trackID_to_name[track['track_uri'].split(\":\")[2].lower()] = track['track_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_tokens, title_tokenizer = tokenize(titles)\n",
    "tracks_tokens, track_tokenizer = tokenize(tracks)\n",
    "\n",
    "title_vocab = len(title_tokenizer.word_index) + 1\n",
    "track_vocab = len(track_tokenizer.word_index) + 1\n",
    "\n",
    "max_title_length = int(len(max(titles_tokens, key=len)))\n",
    "max_track_length = int(len(max(tracks_tokens, key=len)))\n",
    "\n",
    "pad_titles = pad_sequences(titles_tokens, max_title_length, padding = \"post\")\n",
    "pad_tracks = pad_sequences(tracks_tokens, max_track_length, padding = \"post\")\n",
    "\n",
    "pad_titles = pad_titles.reshape(*pad_titles.shape, 1)\n",
    "pad_tracks = pad_tracks.reshape(*pad_tracks.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input(shape=(max_title_length,))\n",
    "embedding = Embedding(input_dim=title_vocab, output_dim=128,)(input_sequence)\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "r_vec = RepeatVector(max_track_length)(encoder)\n",
    "decoder = LSTM(64, return_sequences=True, dropout=0.2)(r_vec)\n",
    "logits = TimeDistributed(Dense(track_vocab))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 9, 128)            285440    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 64)           33024     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 63997)        4159805   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100, 63997)        0         \n",
      "=================================================================\n",
      "Total params: 4,527,677\n",
      "Trainable params: 4,527,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "234/234 [==============================] - 19s 64ms/step - loss: 5.9258 - accuracy: 0.6176\n",
      "Epoch 2/3\n",
      "234/234 [==============================] - 15s 64ms/step - loss: 4.4614 - accuracy: 0.6200\n",
      "Epoch 3/3\n",
      "234/234 [==============================] - 15s 64ms/step - loss: 4.4562 - accuracy: 0.6199\n"
     ]
    }
   ],
   "source": [
    "model_results = enc_dec_model.fit(pad_titles, pad_tracks, batch_size=30, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(logits, tokenizer):\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = 'empty'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argsort(logits, axis=1)[:, -1 * random.randrange(2, 10)]])\n",
    "    #return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "# index = 5679\n",
    "# print(titles[index])\n",
    "# print(logits_to_sentence(enc_dec_model.predict(pad_titles[index])[0], track_tokenizer))\n",
    "# strlog = logits_to_sentence(enc_dec_model.predict(pad_titles[index])[0], track_tokenizer)\n",
    "# s = set()\n",
    "# for i in strlog.split():\n",
    "#     s.add(trackID_to_name[i])\n",
    "# print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Title: summer\n",
      "Predicted Tracklist: 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 5nqbuaeteogdd6hhcre0dz 7yyrtczmciyzzjlnzgc9ol 7kxjtscq5nl1loytl7xaws 0v9wz8o0bt8du38r4ddjeh 0v9wz8o0bt8du38r4ddjeh 0v9wz8o0bt8du38r4ddjeh 0v9wz8o0bt8du38r4ddjeh 62vpwi1chwfy7tmicsstl8 62vpwi1chwfy7tmicsstl8 62vpwi1chwfy7tmicsstl8 62vpwi1chwfy7tmicsstl8 62vpwi1chwfy7tmicsstl8 5xjjdnpkwmbuwe79gv0nxk 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 343ybumqhu19cgogarutsd 343ybumqhu19cgogarutsd 343ybumqhu19cgogarutsd 343ybumqhu19cgogarutsd 1xznggdreh1oqq0xzbwxa3 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2 6gbfpufcjlzwgx4lenp6h2\n",
      "Extracted Track Names: {'HUMBLE.', 'No Problem (feat. Lil Wayne & 2 Chainz)', 'Fake Love', 'No Role Modelz', 'goosebumps', 'Broccoli (feat. Lil Yachty)', '679 (feat. Remy Boyz)', 'Gold Digger', 'One Dance'}\n"
     ]
    }
   ],
   "source": [
    "def predict_tracklist(title, enc_dec_model, title_tokenizer, track_tokenizer, max_title_length, max_track_length):\n",
    "    input_title_tokens = title_tokenizer.texts_to_sequences([title.lower()])\n",
    "    pad_input_title = pad_sequences(input_title_tokens, max_title_length, padding=\"post\")\n",
    "    pad_input_title = pad_input_title.reshape(*pad_input_title.shape, 1)\n",
    "\n",
    "    predictions = enc_dec_model.predict(pad_input_title)[0]\n",
    "\n",
    "    predicted_sentence = logits_to_sentence(predictions, track_tokenizer)\n",
    "\n",
    "    track_names = set()\n",
    "    for track_id in predicted_sentence.split():\n",
    "        track_names.add(trackID_to_name.get(track_id, \"Unknown Track Name\"))\n",
    "\n",
    "    return predicted_sentence, track_names\n",
    "\n",
    "input_title = \"summer\"\n",
    "predicted_sentence, track_names = predict_tracklist(input_title, enc_dec_model, title_tokenizer, track_tokenizer, max_title_length, max_track_length)\n",
    "\n",
    "print(f\"Input Title: {input_title}\")\n",
    "print(f\"Predicted Tracklist: {predicted_sentence}\")\n",
    "print(f\"Extracted Track Names: {track_names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
